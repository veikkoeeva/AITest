{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Model Conversion to FP16\n",
    "This notebook downloads an ONNX model and converts it to FP16 precision using Olive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Veikko\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the ONNX Model\n",
    "Download the ResNet50 model from the ONNX Model Zoo if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model from https://github.com/onnx/models/raw/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx...\n",
      "Model downloaded to artifacts/models/resnet50\\resnet50-v2-7.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Define the model URL and local path.\n",
    "model_url = \"https://github.com/onnx/models/raw/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx\"\n",
    "model_dir = \"artifacts/models/resnet50\"\n",
    "model_name = \"resnet50-v2-7.onnx\"\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Create the directory if it doesn't exist.\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Download the model if it doesn't already exist.\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Downloading the model from {model_url}...\")\n",
    "    urllib.request.urlretrieve(model_url, model_path)\n",
    "    print(f\"Model downloaded to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the Olive Workflow\n",
    "Run the Olive workflow to convert the model to FP16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Running Olive workflow...\n",
      "[2025-11-29 22:26:46,217] [INFO] [run.py:99:run_engine] Running workflow default_workflow\n",
      "[2025-11-29 22:26:46,217] [INFO] [run.py:99:run_engine] Running workflow default_workflow\n",
      "[2025-11-29 22:26:46,302] [INFO] [cache.py:138:__init__] Using cache directory: C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\n",
      "[2025-11-29 22:26:46,302] [INFO] [cache.py:138:__init__] Using cache directory: C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\n",
      "[2025-11-29 22:26:46,306] [DEBUG] [cache.py:274:cache_olive_config] Cached olive config to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\olive_config.json\n",
      "[2025-11-29 22:26:46,306] [DEBUG] [cache.py:274:cache_olive_config] Cached olive config to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\olive_config.json\n",
      "[2025-11-29 22:26:46,313] [WARNING] [accelerator_creator.py:94:_fill_accelerators] The following execution providers are filtered: DmlExecutionProvider. Please add the specific execution provider to the config if you want to enable it. \n",
      "[2025-11-29 22:26:46,313] [WARNING] [accelerator_creator.py:94:_fill_accelerators] The following execution providers are filtered: DmlExecutionProvider. Please add the specific execution provider to the config if you want to enable it. \n",
      "[2025-11-29 22:26:46,314] [INFO] [accelerator_creator.py:100:_fill_accelerators] The accelerator execution providers is not specified for cpu. Use the inferred ones. ['CPUExecutionProvider']\n",
      "[2025-11-29 22:26:46,314] [INFO] [accelerator_creator.py:100:_fill_accelerators] The accelerator execution providers is not specified for cpu. Use the inferred ones. ['CPUExecutionProvider']\n",
      "[2025-11-29 22:26:46,315] [DEBUG] [accelerator_creator.py:143:_check_execution_providers] Supported execution providers for device cpu: [<ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>, <ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>]\n",
      "[2025-11-29 22:26:46,315] [DEBUG] [accelerator_creator.py:143:_check_execution_providers] Supported execution providers for device cpu: [<ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>, <ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>]\n",
      "[2025-11-29 22:26:46,315] [DEBUG] [accelerator_creator.py:179:create_accelerator] Initial accelerators and execution providers: {'cpu': ['CPUExecutionProvider']}\n",
      "[2025-11-29 22:26:46,315] [DEBUG] [accelerator_creator.py:179:create_accelerator] Initial accelerators and execution providers: {'cpu': ['CPUExecutionProvider']}\n",
      "[2025-11-29 22:26:46,316] [INFO] [accelerator_creator.py:195:create_accelerator] Running workflow on accelerator spec: cpu-cpu\n",
      "[2025-11-29 22:26:46,316] [INFO] [accelerator_creator.py:195:create_accelerator] Running workflow on accelerator spec: cpu-cpu\n",
      "[2025-11-29 22:26:46,317] [DEBUG] [cache.py:295:set_cache_env] Set OLIVE_CACHE_DIR: C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\n",
      "[2025-11-29 22:26:46,317] [DEBUG] [cache.py:295:set_cache_env] Set OLIVE_CACHE_DIR: C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\n",
      "[2025-11-29 22:26:46,319] [INFO] [engine.py:206:run] Running Olive on accelerator: cpu-cpu\n",
      "[2025-11-29 22:26:46,319] [INFO] [engine.py:206:run] Running Olive on accelerator: cpu-cpu\n",
      "[2025-11-29 22:26:46,319] [INFO] [engine.py:824:_create_system] Creating target system ...\n",
      "[2025-11-29 22:26:46,319] [INFO] [engine.py:824:_create_system] Creating target system ...\n",
      "[2025-11-29 22:26:46,320] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2025-11-29 22:26:46,320] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2025-11-29 22:26:46,321] [INFO] [engine.py:827:_create_system] Target system created in 0.001220 seconds\n",
      "[2025-11-29 22:26:46,321] [INFO] [engine.py:827:_create_system] Target system created in 0.001220 seconds\n",
      "[2025-11-29 22:26:46,322] [INFO] [engine.py:830:_create_system] Creating host system ...\n",
      "[2025-11-29 22:26:46,322] [INFO] [engine.py:830:_create_system] Creating host system ...\n",
      "[2025-11-29 22:26:46,323] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2025-11-29 22:26:46,323] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2025-11-29 22:26:46,324] [INFO] [engine.py:833:_create_system] Host system created in 0.000994 seconds\n",
      "[2025-11-29 22:26:46,324] [INFO] [engine.py:833:_create_system] Host system created in 0.000994 seconds\n",
      "[2025-11-29 22:26:46,394] [DEBUG] [engine.py:284:run_accelerator] Running Olive in no-search mode ...\n",
      "[2025-11-29 22:26:46,394] [DEBUG] [engine.py:284:run_accelerator] Running Olive in no-search mode ...\n",
      "[2025-11-29 22:26:46,402] [DEBUG] [engine.py:326:_run_no_search] Running ['quantization'] with no search ...\n",
      "[2025-11-29 22:26:46,402] [DEBUG] [engine.py:326:_run_no_search] Running ['quantization'] with no search ...\n",
      "[2025-11-29 22:26:46,404] [INFO] [engine.py:656:_run_pass] Running pass quantization:onnxfloattofloat16\n",
      "[2025-11-29 22:26:46,404] [INFO] [engine.py:656:_run_pass] Running pass quantization:onnxfloattofloat16\n",
      "[2025-11-29 22:27:08,540] [INFO] [engine.py:724:_run_pass] Pass quantization:onnxfloattofloat16 finished in 22.136252 seconds\n",
      "[2025-11-29 22:27:08,540] [INFO] [engine.py:724:_run_pass] Pass quantization:onnxfloattofloat16 finished in 22.136252 seconds\n",
      "[2025-11-29 22:27:08,542] [DEBUG] [cache.py:182:cache_model] Cached model 8c7d2b87 to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\runs\\8c7d2b87\\model.json\n",
      "[2025-11-29 22:27:08,542] [DEBUG] [cache.py:182:cache_model] Cached model 8c7d2b87 to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\runs\\8c7d2b87\\model.json\n",
      "[2025-11-29 22:27:08,545] [DEBUG] [cache.py:224:cache_run] Cached run 8c7d2b87 to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\runs\\8c7d2b87\\run.json\n",
      "[2025-11-29 22:27:08,545] [DEBUG] [cache.py:224:cache_run] Cached run 8c7d2b87 to C:\\projektit\\AITest\\src\\AITest.PythonAI\\cache\\default_workflow\\runs\\8c7d2b87\\run.json\n",
      "[2025-11-29 22:27:08,546] [DEBUG] [engine.py:636:_run_passes] Signal: None, ['8c7d2b87']\n",
      "[2025-11-29 22:27:08,546] [DEBUG] [engine.py:636:_run_passes] Signal: None, ['8c7d2b87']\n",
      "[2025-11-29 22:27:08,547] [INFO] [engine.py:294:run_accelerator] Save footprint to C:\\projektit\\AITest\\src\\AITest.PythonAI\\converted\\MSFT\\converted_fp16_model\\footprint.json.\n",
      "[2025-11-29 22:27:08,547] [INFO] [engine.py:294:run_accelerator] Save footprint to C:\\projektit\\AITest\\src\\AITest.PythonAI\\converted\\MSFT\\converted_fp16_model\\footprint.json.\n",
      "[2025-11-29 22:27:08,549] [DEBUG] [engine.py:296:run_accelerator] run_accelerator done\n",
      "[2025-11-29 22:27:08,549] [DEBUG] [engine.py:296:run_accelerator] run_accelerator done\n",
      "[2025-11-29 22:27:08,550] [INFO] [engine.py:215:run] Run history for cpu-cpu:\n",
      "[2025-11-29 22:27:08,550] [INFO] [engine.py:215:run] Run history for cpu-cpu:\n",
      "[2025-11-29 22:27:08,552] [INFO] [engine.py:472:_dump_run_history] Please install tabulate for better run history output\n",
      "[2025-11-29 22:27:08,552] [INFO] [engine.py:472:_dump_run_history] Please install tabulate for better run history output\n",
      "[2025-11-29 22:27:08,554] [INFO] [engine.py:223:run] Package top ranked 1 models as artifacts\n",
      "[2025-11-29 22:27:08,554] [INFO] [engine.py:223:run] Package top ranked 1 models as artifacts\n",
      "[2025-11-29 22:27:08,555] [INFO] [packaging_generator.py:92:_package_candidate_models] Packaging output models to Zipfile\n",
      "[2025-11-29 22:27:08,555] [INFO] [packaging_generator.py:92:_package_candidate_models] Packaging output models to Zipfile\n",
      "Olive workflow completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from olive.workflows import run as olive_run #type: ignore[import-untyped].\n",
    "\n",
    "# Path to the Olive configuration file.\n",
    "olive_config_path = \"./olive/olive_quantize_cpu_to_fp16.json\"\n",
    "print(os.path.exists(olive_config_path))\n",
    "\n",
    "# Run the Olive workflow.\n",
    "print(\"Running Olive workflow...\")\n",
    "olive_run(olive_config_path)\n",
    "print(\"Olive workflow completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify the Output\n",
    "Check the output directory for the converted FP16 model and the packaged ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files:\n",
      "converted/MSFT/converted_fp16_model\\ConvertedFP16Model.zip\n",
      "converted/MSFT/converted_fp16_model\\footprint.json\n",
      "converted/MSFT/converted_fp16_model\\footprints.json\n",
      "converted/MSFT/converted_fp16_model\\output_footprint.json\n",
      "converted/MSFT/converted_fp16_model\\output_footprints.json\n",
      "converted/MSFT/converted_fp16_model\\output_model\n",
      "converted/MSFT/converted_fp16_model\\run_history.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# List files in the output directory.\n",
    "output_dir = \"converted/MSFT/converted_fp16_model\"\n",
    "output_files = glob.glob(os.path.join(output_dir, \"*\"))\n",
    "\n",
    "if output_files:\n",
    "    print(\"Output files:\")\n",
    "    for file in output_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No output files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9046a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
